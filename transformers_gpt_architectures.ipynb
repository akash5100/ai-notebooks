{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From paper: Improving Language Understanding by generative pre-training\n",
    "\n",
    "# https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-1 Hyperparams \n",
    "\n",
    "bs = 64\n",
    "lr = 2.5e-4\n",
    "warmup = 2000 # warmup, linear increase in lr\n",
    "epoch = 100 # converge\n",
    "\n",
    "context_length = 512\n",
    "SL = context_length # sequence length is better var name\n",
    "vocab_sz = 40_000 # 40k merges\n",
    "emb_sz = 768\n",
    "pos_sz = 768\n",
    "n_head = 12\n",
    "head_sz = 768\n",
    "n_layers = 12 # layers of transformers stacked\n",
    "\n",
    "# regularization\n",
    "P = 0.1 # dropout\n",
    "w = 0.01 # l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer's Core\n",
    "\n",
    "## Self-Attention head (decoder)\n",
    "class Head(nn.Module):\n",
    "  def __init__(self, head_sz):\n",
    "    super().__init__()\n",
    "    self.query = nn.Linear(emb_sz, head_sz, bias=False)\n",
    "    self.key   = nn.Linear(emb_sz, head_sz, bias=False)\n",
    "    self.value = nn.Linear(emb_sz, head_sz, bias=False)\n",
    "    self.register_buffer('tril', torch.tril(torch.ones(SL, SL)))\n",
    "    self.dropout = nn.Dropout(P)\n",
    "  \n",
    "  def forward(self, x):\n",
    "    B,T,C = x.shape\n",
    "    q = self.query(x)\n",
    "    k = self.key(x)\n",
    "    wei = q @ k.transpose(-2,-1) * (head_sz**-0.5)\n",
    "    wei = wei.masked_fill_(self.tril[:T, :T] == 0, float('-inf'))\n",
    "    wei = F.softmax(wei, dim=-1)\n",
    "    wei = self.dropout(wei)\n",
    "    v = self.value(x)\n",
    "    out = wei @ v\n",
    "    return out\n",
    "\n",
    "\n",
    "## Multi-Head Attention\n",
    "class MultiHeadAttention(nn.Module):\n",
    "  def __init__(self, n_head, head_sz):\n",
    "    super().__init__()\n",
    "    self.heads = nn.ModuleList([Head(head_sz) for _ in range(n_head)])\n",
    "    self.linear = nn.Linear(n_head*head_sz, emb_sz)\n",
    "    self.dropout = nn.Dropout(P)\n",
    "  \n",
    "  def forward(self, x): # B,T,C\n",
    "    out = torch.cat([h(x) for h in self.heads], dim=-1) # n* B,T,H -> B,T,n*H\n",
    "    out = self.dropout(self.linear(out)) # B,T,nH @ nH, C -> B,T,C \n",
    "    return out\n",
    "\n",
    "\n",
    "## Feed Forward\n",
    "class FeedForward(nn.Module):\n",
    "  def __init__(self, emb_sz):\n",
    "    super().__init__()\n",
    "    self.net = nn.Sequential(\n",
    "      nn.Linear(emb_sz, emb_sz*4), # emb_sz*4 = 3072 (as mentioned in paper)\n",
    "      nn.LayerNorm(emb_sz*4),\n",
    "      nn.GELU(),\n",
    "      nn.Linear(emb_sz*4, emb_sz),\n",
    "      nn.Dropout(P),\n",
    "    )\n",
    "  \n",
    "  def forward(self, x): # B,T,C\n",
    "    out = self.net(x)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer Block\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "  def __init__(self, n_head, head_sz):\n",
    "    super().__init__()\n",
    "    assert head_sz%n_head == 0 # 768 // 12 -> 64\n",
    "    head_size = head_sz//n_head\n",
    "    self.sa = MultiHeadAttention(n_head, head_size) \n",
    "    self.ff = FeedForward(emb_sz)\n",
    "    self.ln1 = nn.LayerNorm(head_size)\n",
    "    self.ln2 = nn.LayerNorm(head_size)\n",
    "\n",
    "  def forward(self, x): # B, T, C\n",
    "    x = x + self.sa(x)\n",
    "    x = self.ln1(x)\n",
    "    x = x + self.ff(x)\n",
    "    x = self.ln2(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-1\n",
    "# part 1: (unsupervised) learning high capacity language model on a large corpus of text\n",
    "\n",
    "class GPT1(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.vocab_emb = nn.Embedding(vocab_sz, emb_sz)\n",
    "    self.positional_emb = nn.Embedding(SL, emb_sz)\n",
    "    self.blocks = nn.Sequential(*[Transformer(n_head, head_sz) for _ in range(n_layers)])\n",
    "    self.linear = nn.Linear(emb_sz, vocab_sz)\n",
    "    self.apply(self._init_weights)\n",
    " \n",
    "  def _init_weights(self, module):\n",
    "    if isinstance(module, nn.Linear):\n",
    "      torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "      if module.bias is not None:\n",
    "        torch.nn.init.constant_(module.bias, 0.001)\n",
    "    elif isinstance(module, nn.Embedding):\n",
    "      torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "  def forward(self, x): # B,T\n",
    "    B,T = x.shape\n",
    "    tkn_emb = self.vocab_emb(x)\n",
    "    pos_emb = self.positional_emb(torch.arange(T))\n",
    "    x = tkn_emb + pos_emb\n",
    "    x = self.blocks(x)\n",
    "    logits = self.linear(x)\n",
    "    return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.15 billion parameters'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPT1()\n",
    "optim = torch.optim.Adam(model.parameters(), lr, (0.9, 0.995))\n",
    "\n",
    "str(round(sum([p.nelement() for p in model.parameters()]) / 1e9, 2)) + ' billion parameters'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# they achieved 18.4 perplexity with GPT-1 on BookCorpus dataset.\n",
    "\n",
    "# part 2: supervised finetuning stage, where they adapt the model for discriminative task with labeled data, like text classification, entailement, similarity, MCQ.\n",
    "\n",
    "# example of classification\n",
    "# B,T -> (GPT-1) -> B,T,V -> Linear transformation to the number of classes in the classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From paper: Language Models are Unsupervised Multitask Learners (2019)\n",
    "# https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf\n",
    "\n",
    "# demonstrated that the language models can perform down-stream tasks in a zero-shot settingâ€“ without any parameter or architecture modification. highlighting the ability of language models to perform a wide range of tasks in a zero-shot setting.\n",
    "\n",
    "# Verification & sanity check: using n-gram overlap based de-duplication as an important verification step and sanity check during the creation of training and test splits for new NLP datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-2 large Hyperparams \n",
    "\n",
    "bs = 512\n",
    "lr = 2.5e-4\n",
    "warmup = 2000 # warmup, linear increase in lr\n",
    "epoch = 100 # converge\n",
    "\n",
    "context_length = 1024\n",
    "SL = context_length # sequence length is better var name\n",
    "vocab_sz = 50257\n",
    "emb_sz = 1600\n",
    "pos_sz = 1600\n",
    "n_head = 12\n",
    "head_sz = 768\n",
    "n_layers = 48 # layers of transformers stacked\n",
    "\n",
    "# regularization\n",
    "P = 0.1 # dropout\n",
    "w = 0.01 # l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer Block for GPT-2\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "  def __init__(self, n_head, head_sz):\n",
    "    super().__init__()\n",
    "    assert head_sz%n_head == 0 # 768 // 12 -> 64\n",
    "    head_size = head_sz//n_head\n",
    "    self.sa = MultiHeadAttention(n_head, head_size)\n",
    "    self.ff = FeedForward(emb_sz)\n",
    "    self.ln1 = nn.LayerNorm(head_sz)\n",
    "    self.ln2 = nn.LayerNorm(head_sz)\n",
    "\n",
    "  def forward(self, x): # B, T, C\n",
    "    x = x + self.sa(self.ln1(x))\n",
    "    x = x + self.ff(self.ln2(x))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GPT2(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.vocab_emb = nn.Embedding(vocab_sz, emb_sz)\n",
    "    self.positional_emb = nn.Embedding(SL, emb_sz)\n",
    "    self.blocks = nn.Sequential(*[Transformer(n_head, head_sz) for _ in range(n_layers)])\n",
    "    self.lnorm = nn.LayerNorm(emb_sz) # final layer norm\n",
    "    self.linear = nn.Linear(emb_sz, vocab_sz)\n",
    "    self.apply(self._init_weights)\n",
    " \n",
    "  def _init_weights(self, module):\n",
    "    if isinstance(module, nn.Linear):\n",
    "      torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "      if module.bias is not None:\n",
    "        torch.nn.init.constant_(module.bias, 0.001)\n",
    "    elif isinstance(module, nn.Embedding):\n",
    "      torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "  def forward(self, x): # B,T\n",
    "    B,T = x.shape\n",
    "    tkn_emb = self.vocab_emb(x)\n",
    "    pos_emb = self.positional_emb(torch.arange(T))\n",
    "    x = tkn_emb + pos_emb\n",
    "    x = self.blocks(x)\n",
    "    x = self.lnorm(x)\n",
    "    logits = self.linear(x)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.38 billion parameters'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPT2()\n",
    "optim = torch.optim.Adam(model.parameters(), lr, (0.9, 0.995))\n",
    "\n",
    "str(round(sum([p.nelement() for p in model.parameters()]) / 1e9, 2)) + ' billion parameters'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ZeroShot behavior: Why pre-training language model is effective?\n",
    "# Hypothesis, underlying generative model learns to perform many task\n",
    "# in order to improve its language modeling capacity.\n",
    "# The more structured attentional memory of the transformer assists in\n",
    "# transfer compared to LSTMs.\n",
    "\n",
    "# How Zero shot prompts works?\n",
    "# Summarization, they added text (article) and at the end `TL;DR:`\n",
    "# GPT-2 Focuses on recent contents from the article or confuse specific\n",
    "# details such as how many cars were involved in the crash or \n",
    "# whether a logo was on a hat or shirt.\n",
    "\n",
    "# Translation?\n",
    "# prompt:\n",
    "# english sentence = french sentence\n",
    "# <some english sentence> =\n",
    "\n",
    "# Question Answering is a bit interesting and funny, figure out yourself."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fun",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
