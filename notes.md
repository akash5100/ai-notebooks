### some research papers list

protip, use  perplexity.ai as a resource scrapper

- [x] Understanding deeplearning requires rethinking generalization (research paper)-- See mnist_generalization notebook
- [ ] SGD - AdamW / AdamScheduleFree (history)

- [ ] CNN Casestudy: CNN - Le - Alex - ZF - VGG - GoogleNet (inception architecture)
- [ ] Visualizing CNN techniques
- [ ] ResNet (residual and skip connection, research paper)

- [ ] Generating Sequence with RNN (Alex graves, 2013)
- [ ] Seq2Seq (Ilya, 2014)
- [ ] Attention is all you need (Transformer)
  - [ ] Self A, Multi Head A, Cross A (Attention)
  - [ ] BERT, T5
- [ ] https://openai.com/research/weak-to-strong-generalization
- [ ] https://openai.com/research/microscope
- [ ] https://openai.com/research/language-models-can-explain-neurons-in-language-models
- [ ] Mistral of Experts
- [ ] Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention



- [ ] Pixel RNN (maybe if interested)
- [ ] ViT
- [ ] VAE
- [ ] GAN
- [ ] Stable Diffusion

(TODO: add more recently used normalization layers in chronological order, to know how one improved other for eg)

- [ ] LayerNorm (research paper)
- [ ] The recurrent temporal restricted boltzmann machine (research paper)
- [ ] Faster Training: Super Convergence (research paper)
- [ ] Training method: CURRICULUM LEARNING FOR LANGUAGE MODELING (research paper)
